{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE DAY 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE LEVEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama line count =  66\n",
      "Obama word count =  2400\n"
     ]
    }
   ],
   "source": [
    "# 1a\n",
    "with open('C:\\\\Users\\\\user\\\\Downloads\\\\obama_speech.txt', 'r') as obamas_file:\n",
    "        lines = obamas_file.readlines()\n",
    "        line_count = len(lines)\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "print('Obama line count = ', line_count)\n",
    "print('Obama word count = ', word_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michelle_Obama line count =  83\n",
      "Michelle_Obama word count =  2204\n"
     ]
    }
   ],
   "source": [
    "# 1b\n",
    "with open('C:\\\\Users\\\\user\\\\Downloads\\\\michelle_obama_speech.txt', 'r') as michelle_obamas_file:\n",
    "        lines = michelle_obamas_file.readlines()\n",
    "        line_count = len(lines)\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "print('Michelle_Obama line count = ', line_count)\n",
    "print('Michelle_Obama word count = ', word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald line count =  48\n",
      "Donald word count =  1259\n"
     ]
    }
   ],
   "source": [
    "# 1c\n",
    "with open('C:\\\\Users\\\\user\\\\Downloads\\\\donald_speech.txt', 'r') as donald_file:\n",
    "        lines = donald_file.readlines()\n",
    "        line_count = len(lines)\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "print('Donald line count = ', line_count)\n",
    "print('Donald word count = ', word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melina Trump line count =  33\n",
      "Melina Trump word count =  1375\n"
     ]
    }
   ],
   "source": [
    "# 1d\n",
    "with open('C:\\\\Users\\\\user\\\\Downloads\\\\melina_trump_speech.txt', 'r') as melina_trump_file:\n",
    "        lines = melina_trump_file.readlines()\n",
    "        line_count = len(lines)\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "print('Melina Trump line count = ', line_count)\n",
    "print('Melina Trump word count = ', word_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 9), ('Russian', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Serbian', 4)]\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "import json\n",
    "def most_spoken_languages(filename, n):\n",
    "   with open('C:\\\\Users\\\\user\\\\Downloads\\\\countries_data.json', 'r') as countries_data_json:\n",
    "      data = json.load(countries_data_json)\n",
    "    #create a dictionary to hold the language count\n",
    "      languages = {}\n",
    "      for country in data:\n",
    "          for language in country['languages']:\n",
    "              if language in languages:\n",
    "                  languages[language] += 1\n",
    "              else:\n",
    "                languages[language] = 1\n",
    "      sorted_languages = sorted(languages.items(), key=lambda x: x[1], reverse = True)\n",
    "      return sorted_languages[:n]\n",
    "print(most_spoken_languages(filename = 'countries_data_json', n = 10))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'China', 'capital': 'Beijing', 'languages': ['Chinese'], 'population': 1377422166, 'flag': 'https://restcountries.eu/data/chn.svg', 'currency': 'Chinese yuan'}, {'name': 'India', 'capital': 'New Delhi', 'languages': ['Hindi', 'English'], 'population': 1295210000, 'flag': 'https://restcountries.eu/data/ind.svg', 'currency': 'Indian rupee'}, {'name': 'United States of America', 'capital': 'Washington, D.C.', 'languages': ['English'], 'population': 323947000, 'flag': 'https://restcountries.eu/data/usa.svg', 'currency': 'United States dollar'}, {'name': 'Indonesia', 'capital': 'Jakarta', 'languages': ['Indonesian'], 'population': 258705000, 'flag': 'https://restcountries.eu/data/idn.svg', 'currency': 'Indonesian rupiah'}, {'name': 'Brazil', 'capital': 'Bras√≠lia', 'languages': ['Portuguese'], 'population': 206135893, 'flag': 'https://restcountries.eu/data/bra.svg', 'currency': 'Brazilian real'}, {'name': 'Pakistan', 'capital': 'Islamabad', 'languages': ['English', 'Urdu'], 'population': 194125062, 'flag': 'https://restcountries.eu/data/pak.svg', 'currency': 'Pakistani rupee'}, {'name': 'Nigeria', 'capital': 'Abuja', 'languages': ['English'], 'population': 186988000, 'flag': 'https://restcountries.eu/data/nga.svg', 'currency': 'Nigerian naira'}, {'name': 'Bangladesh', 'capital': 'Dhaka', 'languages': ['Bengali'], 'population': 161006790, 'flag': 'https://restcountries.eu/data/bgd.svg', 'currency': 'Bangladeshi taka'}, {'name': 'Russian Federation', 'capital': 'Moscow', 'languages': ['Russian'], 'population': 146599183, 'flag': 'https://restcountries.eu/data/rus.svg', 'currency': 'Russian ruble'}, {'name': 'Japan', 'capital': 'Tokyo', 'languages': ['Japanese'], 'population': 126960000, 'flag': 'https://restcountries.eu/data/jpn.svg', 'currency': 'Japanese yen'}]\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "import json\n",
    "def get_ten_most_populated_countries(filename, n):\n",
    "    with open('C:\\\\Users\\\\user\\\\Downloads\\\\countries_data.json', 'r') as countries_data_json:\n",
    "       data = json.load(countries_data_json)\n",
    "       sorted_countries = sorted(data, key=lambda x: x['population'], reverse= True)\n",
    "       return sorted_countries[:n]\n",
    "print(get_ten_most_populated_countries(filename = 'countries_data_json', n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE LEVEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "import re\n",
    "def extract_incoming_email_addresses():\n",
    "    with open('C:\\\\Users\\\\user\\\\Downloads\\\\email_exchanges_big.txt') as file:\n",
    "        content = file.read()\n",
    "        regex_pattern = r'From:.*<(\\S+@\\S+)>'\n",
    "        email_addresses = re.findall(regex_pattern, content)\n",
    "        return email_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "def find_most_common_word(text, num_words):\n",
    "    text = 'Ada is as good'\n",
    "    return find_most_common_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function find_most_common_word at 0x000002119EABC160>\n",
      "<function find_most_common_word at 0x000002119EABC160>\n",
      "<function find_most_common_word at 0x000002119EABC160>\n",
      "<function find_most_common_word at 0x000002119EABC160>\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "print(find_most_common_word(obamas_file, 10))\n",
    "print(find_most_common_word(michelle_obamas_file, 10))\n",
    "print(find_most_common_word(melina_trump_file, 10))\n",
    "print(find_most_common_word(donald_file, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two speeches: 68.36%\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "import string\n",
    "import math\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize the text\n",
    "    return text.split()\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set([\n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "        \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "        \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    "        \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "        \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "        \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "        \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "        \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\",\n",
    "        \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\",\n",
    "        \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
    "    ])\n",
    "    # Filter out stop words\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def calculate_tf(term, document):\n",
    "    return document.count(term)\n",
    "\n",
    "def calculate_idf(term, documents):\n",
    "    document_count = sum(1 for doc in documents if term in doc)\n",
    "    return math.log(len(documents) / (1 + document_count))\n",
    "\n",
    "def check_text_similarity(text1, text2):\n",
    "    # Tokenize and remove stop words from the input texts\n",
    "    tokens1 = remove_stop_words(clean_and_tokenize(text1))\n",
    "    tokens2 = remove_stop_words(clean_and_tokenize(text2))\n",
    "\n",
    "    # Combine tokens back into strings\n",
    "    cleaned_text1 = \" \".join(tokens1)\n",
    "    cleaned_text2 = \" \".join(tokens2)\n",
    "\n",
    "    # Create a list of documents\n",
    "    documents = [cleaned_text1, cleaned_text2]\n",
    "\n",
    "    # Calculate TF-IDF manually\n",
    "    tfidf_matrix = []\n",
    "    for term in set(tokens1 + tokens2):\n",
    "        tfidf1 = calculate_tf(term, tokens1) * calculate_idf(term, documents)\n",
    "        tfidf2 = calculate_tf(term, tokens2) * calculate_idf(term, documents)\n",
    "        tfidf_matrix.append((tfidf1, tfidf2))\n",
    "\n",
    "    # Calculate cosine similarity between the two texts\n",
    "    dot_product = sum(tfidf1 * tfidf2 for tfidf1, tfidf2 in tfidf_matrix)\n",
    "    magnitude1 = math.sqrt(sum(tfidf1 ** 2 for tfidf1, _ in tfidf_matrix))\n",
    "    magnitude2 = math.sqrt(sum(tfidf2 ** 2 for _, tfidf2 in tfidf_matrix))\n",
    "    similarity_score = dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    file1_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\michelle_obama_speech.txt\"  \n",
    "    file2_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\melina_trump_speech.txt\"    \n",
    "\n",
    "    # Read the content of the files\n",
    "    with open(file1_path, 'r') as file:\n",
    "        michelle_speech = file.read()\n",
    "\n",
    "    with open(file2_path, 'r') as file:\n",
    "        melina_speech = file.read()\n",
    "\n",
    "    # Check the similarity between the two speeches\n",
    "    similarity = check_text_similarity(michelle_speech, melina_speech)\n",
    "\n",
    "    print(f\"Similarity between the two speeches: {similarity:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most repeated words in 'romeo_and_juliet.txt':\n",
      "the: 863 occurrences\n",
      "and: 792 occurrences\n",
      "to: 624 occurrences\n",
      "i: 573 occurrences\n",
      "of: 535 occurrences\n",
      "a: 520 occurrences\n",
      "in: 375 occurrences\n",
      "is: 373 occurrences\n",
      "that: 363 occurrences\n",
      "my: 359 occurrences\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "from collections import Counter\n",
    "\n",
    "def get_most_common_words(file_path, num_words=10):\n",
    "    with open('C:\\\\Users\\\\user\\\\Downloads\\\\romeo_and_juliet.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower().strip(\".,!?()[]{}:\\\"\") for word in words]\n",
    "\n",
    "    # Count the occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the most common words\n",
    "    most_common_words = word_counts.most_common(num_words)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\romeo_and_juliet.txt\"  \n",
    "\n",
    "    most_common_words = get_most_common_words(file_path)\n",
    "\n",
    "    print(\"10 most repeated words in 'romeo_and_juliet.txt':\")\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count} occurrences\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def count_lines(file_path, keyword):\n",
    "    with open('C:\\\\Users\\\\user\\\\Downloads\\\\hacker_news.csv', 'r') as file:\n",
    "        reader = csv.reader(file, delimiter= ',')\n",
    "        count = sum(1 for row in reader if keyword in row['title'])\n",
    "\n",
    "    return count\n",
    "\n",
    "def count_python_lines(file_path):\n",
    "    return count_lines(file_path, 'python') + count_lines(file_path, 'Python')\n",
    "\n",
    "def count_javascript_lines(file_path):\n",
    "    return count_lines(file_path, 'JavaScript') + count_lines(file_path, 'javascript') + count_lines(file_path, 'Javascript')\n",
    "\n",
    "def count_java_not_javascript_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        count = sum(1 for row in reader if 'java' in row['title'].lower() and 'javascript' not in row['title'].lower())\n",
    "\n",
    "    return count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:\\\\Users\\\\user\\\\Downloads\\\\hacker_news.csv\"  \n",
    "\n",
    "    # Count the lines containing Python or Python\n",
    "    python_count = count_python_lines(file_path)\n",
    "    print(f\"Number of lines containing 'python' or 'Python': {python_count}\")\n",
    "\n",
    "    # Count the lines containing JavaScript, javascript, or Javascript\n",
    "    javascript_count = count_javascript_lines(file1_path)\n",
    "    print(f\"Number of lines containing 'JavaScript', 'javascript', or 'Javascript': {javascript_count}\")\n",
    "\n",
    "    # Count the lines containing Java and not JavaScript\n",
    "    java_not_javascript_count = count_java_not_javascript_lines(file1_path)\n",
    "    print(f\"Number of lines containing 'Java' and not 'JavaScript': {java_not_javascript_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArewaDSworkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
